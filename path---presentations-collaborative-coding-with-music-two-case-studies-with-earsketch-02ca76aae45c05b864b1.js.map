{"version":3,"sources":["webpack:///path---presentations-collaborative-coding-with-music-two-case-studies-with-earsketch-02ca76aae45c05b864b1.js","webpack:///./.cache/json/presentations-collaborative-coding-with-music-two-case-studies-with-earsketch.json"],"names":["webpackJsonp","469","module","exports","pathContext","abstract","authors","name","slug","title","type"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,aAAeC,SAAA,ohBAAAC,UAA2iBC,KAAA,oBAA2BA,KAAA,sBAA6BA,KAAA,kBAAuBC,KAAA,kEAAAC,MAAA,mEAAAC,KAAA","file":"path---presentations-collaborative-coding-with-music-two-case-studies-with-earsketch-02ca76aae45c05b864b1.js","sourcesContent":["webpackJsonp([252725346515905],{\n\n/***/ 469:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"pathContext\":{\"abstract\":\"This paper describes the motivation, design, and implementation of new features in EarSketch that enable the collaborative creation of algorithmic music. EarSketch is a web-based Digital Audio Workstation (DAW), designed primarily for educational contexts, in which users author Python or JavaScript code to programmatically create music within a multi-track paradigm. In this paper, we describe these new collaborative features in EarSketch and discuss their potential for use in both educational and music performance contexts.\",\"authors\":[{\"name\":\"Avneesh Sarwate\"},{\"name\":\"Takahiko Tsuchiya\"},{\"name\":\"Jason Freeman\"}],\"slug\":\"collaborative-coding-with-music-two-case-studies-with-earsketch\",\"title\":\"Collaborative Coding with Music: Two Case Studies with EarSketch\",\"type\":\"paper\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---presentations-collaborative-coding-with-music-two-case-studies-with-earsketch-02ca76aae45c05b864b1.js","module.exports = {\"pathContext\":{\"abstract\":\"This paper describes the motivation, design, and implementation of new features in EarSketch that enable the collaborative creation of algorithmic music. EarSketch is a web-based Digital Audio Workstation (DAW), designed primarily for educational contexts, in which users author Python or JavaScript code to programmatically create music within a multi-track paradigm. In this paper, we describe these new collaborative features in EarSketch and discuss their potential for use in both educational and music performance contexts.\",\"authors\":[{\"name\":\"Avneesh Sarwate\"},{\"name\":\"Takahiko Tsuchiya\"},{\"name\":\"Jason Freeman\"}],\"slug\":\"collaborative-coding-with-music-two-case-studies-with-earsketch\",\"title\":\"Collaborative Coding with Music: Two Case Studies with EarSketch\",\"type\":\"paper\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/presentations-collaborative-coding-with-music-two-case-studies-with-earsketch.json\n// module id = 469\n// module chunks = 252725346515905"],"sourceRoot":""}